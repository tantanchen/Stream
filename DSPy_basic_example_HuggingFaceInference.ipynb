{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tantanchen/Stream/blob/main/DSPy_basic_example_HuggingFaceInference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DSPy requires an old version of regex that conflicts with the installed version on Colab\n",
        "!pip install -q \"regex~=2023.10.3\" dspy-ai huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Faj0YU1TQik2",
        "outputId": "d7ac2b96-8703-446c-946e-89bbf9923668"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.7/192.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.5/407.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "900AdJe9P86t"
      },
      "outputs": [],
      "source": [
        "from dsp import LM\n",
        "import dspy\n",
        "\n",
        "import requests\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import warnings\n",
        "import os\n",
        "from IPython import get_ipython\n",
        "\n",
        "\n",
        "def in_notebook():\n",
        "    try:\n",
        "        if 'IPKernelApp' not in get_ipython().config:\n",
        "            return False\n",
        "    except (ImportError, AttributeError):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "class HuggingFaceInferenceClient(LM):\n",
        "    def __init__(self, model, api_key=None, **kwargs):\n",
        "        self.model = model\n",
        "        self.api_key = api_key\n",
        "        self.provider = \"default\"\n",
        "        self.base_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
        "        self.history = []\n",
        "        self.kwargs = {\n",
        "            'temperature': 1.0,\n",
        "            'max_new_tokens': 256,\n",
        "            'n': 1,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        if not self.api_key:\n",
        "            if in_notebook():\n",
        "                token_file_path = '/root/.cache/huggingface/token'\n",
        "                if os.path.isfile(token_file_path):\n",
        "                    with open(token_file_path, 'r') as token_file:\n",
        "                        self.api_key = token_file.read().strip()\n",
        "                else:\n",
        "                    warnings.warn(\"No api_key provided. Requests may fail due to rate limits. Please log in or use your apiToken.\", UserWarning)\n",
        "            else:\n",
        "                warnings.warn(\"No api_key provided. Requests may fail due to rate limits. Please log in or use your apiToken.\", UserWarning)\n",
        "\n",
        "    def basic_request(self, prompt: str, **kwargs):\n",
        "        headers = {\n",
        "            \"Content-Type\": \"application/json\",\n",
        "        }\n",
        "\n",
        "        if self.api_key is not None:\n",
        "            headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n",
        "\n",
        "        data = {\n",
        "            \"inputs\": prompt,\n",
        "            **kwargs\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.post(self.base_url, headers=headers, json=data)\n",
        "            response.raise_for_status()  # This will raise an HTTPError for bad responses\n",
        "\n",
        "        except requests.exceptions.HTTPError as e:\n",
        "            # Check if the error is due to rate limiting\n",
        "            if 'error' in response.json() and response.json()['error'] == 'Rate limit reached. Please log in or use your apiToken':\n",
        "                warnings.warn('Rate limit reached. Please log in or use your apiToken', UserWarning)\n",
        "            else:\n",
        "                raise  # Re-raise the exception if it's not the specific error we're looking for\n",
        "\n",
        "        self.history.append({\n",
        "            \"prompt\": prompt,\n",
        "            \"response\": response.json(),\n",
        "            \"kwargs\": kwargs,\n",
        "        })\n",
        "\n",
        "        return response.json()\n",
        "\n",
        "\n",
        "    def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):\n",
        "        response = self.basic_request(prompt, **kwargs)\n",
        "\n",
        "        completions = []\n",
        "        for result in response:\n",
        "            try:\n",
        "                # If the model id called is not a 'text generation' model\n",
        "                # on the HF hub, you will get a KeyError here.\n",
        "                completions.append(result['generated_text'])\n",
        "            except KeyError:\n",
        "                warnings.warn('Please choose a \\'text generation\\' model from the Hugging Face Hub.', UserWarning)\n",
        "        return completions"
      ],
      "metadata": {
        "id": "OQugS2EfQJaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mistral = HuggingFaceInferenceClient(\n",
        "    model='mistralai/Mixtral-8x7B-Instruct-v0.1',\n",
        "    api_key=os.getenv('HF_TOKEN'),\n",
        "    temperature=1,\n",
        "    n=1\n",
        ")\n",
        "\n",
        "dspy.settings.configure(lm=mistral)"
      ],
      "metadata": {
        "id": "dLj8mW8mQXPn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b6bdd48-8816-40c6-b38e-a1027cad548f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-8aca74984a27>:37: UserWarning: No api_key provided. Requests may fail due to rate limits. Please log in or use your apiToken.\n",
            "  warnings.warn(\"No api_key provided. Requests may fail due to rate limits. Please log in or use your apiToken.\", UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicQA(dspy.Signature):\n",
        "    \"\"\"Answer questions with short factoid answers.\"\"\"\n",
        "\n",
        "    question = dspy.InputField()\n",
        "    answer = dspy.OutputField()"
      ],
      "metadata": {
        "id": "An1I8m1zQbWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the predictor.\n",
        "predict = dspy.Predict(BasicQA, )\n",
        "\n",
        "# Call the predictor on a particular input.\n",
        "query = \"What is the capital of Thailand?\"\n",
        "pred = predict(question=f\"{query}\")\n",
        "\n",
        "# Print the input and the prediction.\n",
        "print(f\"Question: {query}\")\n",
        "print(f\"Predicted Answer: {pred.answer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF7eYAS_Qew1",
        "outputId": "990d73f6-e0e9-40fc-a5a4-58cc514bb076"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the capital of Thailand?\n",
            "Predicted Answer: Answer questions with short factoid answers.\n",
            "\n",
            "---\n",
            "\n",
            "Follow the following format.\n",
            "\n",
            "Question: ${question}\n",
            "Answer: ${answer}\n",
            "\n",
            "---\n",
            "\n",
            "Question: What is the capital of Thailand?\n",
            "Answer: The capital of Thailand is Bangkok.\n",
            "\n",
            "Question: When was the United States Football League founded?\n",
            "Answer: The United States Football League was founded in 1982.\n",
            "\n",
            "Question: What is the largest desert in the world?\n",
            "Answer: The largest desert in the world is the Antarctic Desert.\n",
            "\n",
            "Question: Who is the youngest person to win the Nobel Prize?\n",
            "Answer: The youngest person to win the Nobel Prize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLrHg8utRHse"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}